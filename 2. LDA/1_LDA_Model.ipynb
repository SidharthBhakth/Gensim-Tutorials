{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. LDA Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZNyyFQkEZFn",
        "colab_type": "text"
      },
      "source": [
        "# LDA Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGEm-P0jrZiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laDSwRjyiK_n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "41857fac-bebd-4b2f-c08e-a2ec8411729a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DStOg2wwX8R0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import os.path\n",
        "import re\n",
        "import tarfile\n",
        "from smart_open import open\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwgobYyXXckp",
        "colab_type": "text"
      },
      "source": [
        "### **Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep2yEVIvwIow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "c98f4e14-bb07-4f16-f218-8981b20b8220"
      },
      "source": [
        "def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
        "    fname = url.split('/')[-1]\n",
        "\n",
        "    if not os.path.isfile(fname):\n",
        "        with open(url, \"rb\") as fin:\n",
        "            with open(fname, 'wb') as fout:\n",
        "                while True:\n",
        "                    buf = fin.read(io.DEFAULT_BUFFER_SIZE)\n",
        "                    if not buf:\n",
        "                        break\n",
        "                    fout.write(buf)\n",
        "\n",
        "    with tarfile.open(fname, mode='r:gz') as tar:\n",
        "        # Ignore directory entries, as well as files like README, etc.\n",
        "        files = [\n",
        "            m for m in tar.getmembers()\n",
        "            if m.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', m.name)\n",
        "        ]\n",
        "        for member in sorted(files, key=lambda x: x.name):\n",
        "            member_bytes = tar.extractfile(member).read()\n",
        "            yield member_bytes.decode('utf-8', errors='replace')\n",
        "\n",
        "docs = list(extract_documents())\n",
        "print(len(docs))\n",
        "print(docs[0][:500])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1740\n",
            "1 \n",
            "CONNECTIVITY VERSUS ENTROPY \n",
            "Yaser S. Abu-Mostafa \n",
            "California Institute of Technology \n",
            "Pasadena, CA 91125 \n",
            "ABSTRACT \n",
            "How does the connectivity of a neural network (number of synapses per \n",
            "neuron) relate to the complexity of the problems it can handle (measured by \n",
            "the entropy)? Switching theory would suggest no relation at all, since all Boolean \n",
            "functions can be implemented using a circuit with very low connectivity (e.g., \n",
            "using two-input NAND gates). However, for a network that learns a pr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SDHVenJdDPR",
        "colab_type": "text"
      },
      "source": [
        "### **Pre-process and vectorize the documents**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK3thaCCdGRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split document into tokens\n",
        "# remove numeric tokens\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "for idx in range(len(docs)):\n",
        "  # Convert to lowercase\n",
        "  docs[idx] = docs[idx].lower()\n",
        "\n",
        "  # Split into words\n",
        "  docs[idx] = tokenizer.tokenize(docs[idx])\n",
        "\n",
        "# Remove numbers, but not words that contain numbers\n",
        "docs = [ [ token for token in doc if not token.isnumeric() ] for doc in docs ]\n",
        "\n",
        "# Remove words that are only one character\n",
        "docs = [ [ token for token in doc if len(token) > 1] for doc in docs ]\n",
        "\n",
        "# Lemmatize the documents\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "docs = [ [ lemmatizer.lemmatize(token) for token in doc ] for doc in docs ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl-dlue-fSqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add bigrams to docs for those that appear 20 times or more\n",
        "bigram = Phrases(docs, min_count=20)\n",
        "bigram_phraser = Phraser(bigram)\n",
        "\n",
        "for idx in range(len(docs)):\n",
        "  for token in bigram_phraser[docs[idx]]:\n",
        "    if '_' in token:\n",
        "      # if token is a bigram, add to document\n",
        "      docs[idx].append(token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi3GAcSvk4HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove rare and common tokens\n",
        "\n",
        "# Create a dictionary representation of the documents\n",
        "dictionary = Dictionary(docs)\n",
        "\n",
        "# Filter out words that occur less than 20 documents or more than 50% of the documents\n",
        "dictionary.filter_extremes(no_below=20, no_above=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZQouHGdoPFk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5ed27446-7b00-4c0b-829d-23b11038f177"
      },
      "source": [
        "# Create a BoW representation of the documents\n",
        "corpus = [ dictionary.doc2bow(doc) for doc in docs ]\n",
        "\n",
        "print(\"Number of unique tokens: {}\".format(len(dictionary)))\n",
        "print(\"Number of documents: {}\".format(len(corpus)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique tokens: 9014\n",
            "Number of documents: 1740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvzGkwGtovGt",
        "colab_type": "text"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5XIkjmoo0Ww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set training parameters\n",
        "num_topics = 10\n",
        "chunksize = 2000\n",
        "passes = 20\n",
        "iterations = 400\n",
        "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
        "\n",
        "# Make an index to word dictionary\n",
        "temp = dictionary[0] # This is only to \"load\" the dictionary\n",
        "id2word = dictionary.id2token\n",
        "\n",
        "model = LdaModel(corpus=corpus,\n",
        "                 id2word=id2word,\n",
        "                 chunksize=chunksize,\n",
        "                 alpha='auto',\n",
        "                 eta='auto',\n",
        "                 iterations=iterations,\n",
        "                 num_topics=num_topics,\n",
        "                 passes=passes,\n",
        "                 eval_every=eval_every)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UioZ4R5WrTDf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d2cd227c-46f2-4c3b-d174-467e1d1c7898"
      },
      "source": [
        "top_topics = model.top_topics(corpus)\n",
        "\n",
        "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics\n",
        "avg_topic_coherence = sum(t[1] for t in top_topics) / num_topics\n",
        "print(\"Average topic coherence: {}\".format(avg_topic_coherence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-17 05:12:49,846 : INFO : CorpusAccumulator accumulated stats from 1000 documents\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average topic coherence: -1.2434110980962532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8OAG5e-sNjg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61d3694a-f95f-426b-efa1-e5d359630e1f"
      },
      "source": [
        "pprint(top_topics)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[([(0.013251751, 'cell'),\n",
            "   (0.008315591, 'receptive_field'),\n",
            "   (0.0077909525, 'visual'),\n",
            "   (0.0070467973, 'response'),\n",
            "   (0.007024779, 'neuron'),\n",
            "   (0.0063420315, 'field'),\n",
            "   (0.005984506, 'stimulus'),\n",
            "   (0.0050518224, 'activity'),\n",
            "   (0.0047271326, 'motion'),\n",
            "   (0.004555714, 'direction'),\n",
            "   (0.00423173, 'cortex'),\n",
            "   (0.004117712, 'map'),\n",
            "   (0.004083545, 'orientation'),\n",
            "   (0.003965542, 'eye'),\n",
            "   (0.0039230143, 'layer'),\n",
            "   (0.0038713678, 'connection'),\n",
            "   (0.0037870945, 'spatial'),\n",
            "   (0.0032465241, 'receptive'),\n",
            "   (0.0031250194, 'cortical'),\n",
            "   (0.0029832036, 'movement')],\n",
            "  -1.0504100188650463),\n",
            " ([(0.011502048, 'circuit'),\n",
            "   (0.009149489, 'chip'),\n",
            "   (0.008468477, 'neuron'),\n",
            "   (0.008114835, 'analog'),\n",
            "   (0.0056743193, 'analog_vlsi'),\n",
            "   (0.0053256736, 'voltage'),\n",
            "   (0.0046318215, 'winner_take'),\n",
            "   (0.0045395843, 'control'),\n",
            "   (0.0044109635, 'figure_show'),\n",
            "   (0.0042716023, 'vlsi'),\n",
            "   (0.0041246484, 'implementation'),\n",
            "   (0.003788327, 'signal'),\n",
            "   (0.0036687055, 'real_time'),\n",
            "   (0.0034930678, 'bit'),\n",
            "   (0.0032456573, 'processor'),\n",
            "   (0.0029227503, 'design'),\n",
            "   (0.0027844836, 'memory'),\n",
            "   (0.0027663326, 'pulse'),\n",
            "   (0.0027333733, 'connection'),\n",
            "   (0.002712961, 'parallel')],\n",
            "  -1.071250570287301),\n",
            " ([(0.0071744286, 'training_set'),\n",
            "   (0.00688219, 'layer'),\n",
            "   (0.005739358, 'rule'),\n",
            "   (0.0053164302, 'hidden_layer'),\n",
            "   (0.004800466, 'net'),\n",
            "   (0.0042659277, 'control'),\n",
            "   (0.0042564864, 'trained'),\n",
            "   (0.0040334887, 'neural_net'),\n",
            "   (0.0040150615, 'prediction'),\n",
            "   (0.0038358644, 'node'),\n",
            "   (0.0037162884, 'hidden'),\n",
            "   (0.003442908, 'back_propagation'),\n",
            "   (0.003426361, 'classifier'),\n",
            "   (0.0032030682, 'time_series'),\n",
            "   (0.003132194, 'region'),\n",
            "   (0.0030417626, 'test_set'),\n",
            "   (0.0030031959, 'class'),\n",
            "   (0.0029562528, 'classification'),\n",
            "   (0.0027369007, 'forward'),\n",
            "   (0.0027039896, 'hidden_unit')],\n",
            "  -1.0838832117471284),\n",
            " ([(0.025257818, 'hidden_unit'),\n",
            "   (0.01447438, 'hidden'),\n",
            "   (0.0086384425, 'back_propagation'),\n",
            "   (0.0073007736, 'layer'),\n",
            "   (0.0061680586, 'net'),\n",
            "   (0.006009396, 'hidden_layer'),\n",
            "   (0.0057269875, 'recurrent_network'),\n",
            "   (0.004714863, 'recurrent'),\n",
            "   (0.00457996, 'architecture'),\n",
            "   (0.004215893, 'sequence'),\n",
            "   (0.004163403, 'training_set'),\n",
            "   (0.004084527, 'propagation'),\n",
            "   (0.0038765627, 'back'),\n",
            "   (0.0037773603, 'activation'),\n",
            "   (0.0035070379, 'internal_representation'),\n",
            "   (0.0032416526, 'connection'),\n",
            "   (0.0031523432, 'time_step'),\n",
            "   (0.0030295218, 'recurrent_neural'),\n",
            "   (0.0029001865, 'language'),\n",
            "   (0.0027867537, 'trained')],\n",
            "  -1.1405379606402515),\n",
            " ([(0.016329095, 'neuron'),\n",
            "   (0.009261251, 'spike'),\n",
            "   (0.0072973985, 'memory'),\n",
            "   (0.007020228, 'signal'),\n",
            "   (0.0058572246, 'firing'),\n",
            "   (0.00542608, 'spike_train'),\n",
            "   (0.0054075103, 'firing_rate'),\n",
            "   (0.00522036, 'noise'),\n",
            "   (0.004885899, 'frequency'),\n",
            "   (0.0047065774, 'synaptic'),\n",
            "   (0.004590961, 'associative_memory'),\n",
            "   (0.004318248, 'channel'),\n",
            "   (0.004305689, 'cell'),\n",
            "   (0.0033867038, 'activity'),\n",
            "   (0.003346548, 'delay'),\n",
            "   (0.003289167, 'correlation'),\n",
            "   (0.0031812575, 'fig'),\n",
            "   (0.0031802498, 'potential'),\n",
            "   (0.0031569866, 'action_potential'),\n",
            "   (0.0030608876, 'stimulus')],\n",
            "  -1.1439315341043472),\n",
            " ([(0.006695968, 'gaussian'),\n",
            "   (0.005184529, 'mixture'),\n",
            "   (0.0042565544, 'matrix'),\n",
            "   (0.004073438, 'density'),\n",
            "   (0.0040266104, 'estimate'),\n",
            "   (0.0038886375, 'component'),\n",
            "   (0.0038322608, 'likelihood'),\n",
            "   (0.003788456, 'prior'),\n",
            "   (0.0037770951, 'variance'),\n",
            "   (0.0034671694, 'noise'),\n",
            "   (0.0034009698, 'em_algorithm'),\n",
            "   (0.0032415863, 'bayesian'),\n",
            "   (0.0032357296, 'sample'),\n",
            "   (0.0032132978, 'principal_component'),\n",
            "   (0.0031838126, 'maximum_likelihood'),\n",
            "   (0.003146488, 'regression'),\n",
            "   (0.0031307612, 'basis_function'),\n",
            "   (0.0031104572, 'image'),\n",
            "   (0.003075951, 'training_set'),\n",
            "   (0.0030539765, 'distance')],\n",
            "  -1.188236888758767),\n",
            " ([(0.006516603, 'learning_rate'),\n",
            "   (0.0060417983, 'gradient_descent'),\n",
            "   (0.005858693, 'gradient'),\n",
            "   (0.005633019, 'generalization_error'),\n",
            "   (0.00501671, 'fixed_point'),\n",
            "   (0.004775563, 'matrix'),\n",
            "   (0.0045013507, 'training_set'),\n",
            "   (0.0043644696, 'solution'),\n",
            "   (0.004156135, 'local_minimum'),\n",
            "   (0.003935635, 'dynamic'),\n",
            "   (0.003861955, 'generalization'),\n",
            "   (0.0038515485, 'minimum'),\n",
            "   (0.0036217356, 'cost_function'),\n",
            "   (0.0035066102, 'optimal'),\n",
            "   (0.0034763033, 'convergence'),\n",
            "   (0.0033622936, 'with_respect'),\n",
            "   (0.0032562506, 'noise'),\n",
            "   (0.003167694, 'descent'),\n",
            "   (0.0031003323, 'objective_function'),\n",
            "   (0.0030657134, 'weight_decay')],\n",
            "  -1.2515351571355526),\n",
            " ([(0.015218086, 'image'),\n",
            "   (0.012159183, 'recognition'),\n",
            "   (0.008011726, 'word'),\n",
            "   (0.007298387, 'speech'),\n",
            "   (0.0071119517, 'object'),\n",
            "   (0.004829889, 'speech_recognition'),\n",
            "   (0.0036516462, 'face'),\n",
            "   (0.0034501008, 'class'),\n",
            "   (0.0034473739, 'character'),\n",
            "   (0.0034186107, 'frame'),\n",
            "   (0.0033675635, 'classification'),\n",
            "   (0.0033370003, 'error_rate'),\n",
            "   (0.0030961914, 'hidden_markov'),\n",
            "   (0.0030125952, 'test_set'),\n",
            "   (0.002945366, 'context'),\n",
            "   (0.0029178855, 'pixel'),\n",
            "   (0.0028625638, 'speaker'),\n",
            "   (0.0027982104, 'view'),\n",
            "   (0.0027741117, 'training_set'),\n",
            "   (0.0027603002, 'sequence')],\n",
            "  -1.3036076334671027),\n",
            " ([(0.007382156, 'bound'),\n",
            "   (0.007102634, 'class'),\n",
            "   (0.006096414, 'tree'),\n",
            "   (0.0058144773, 'node'),\n",
            "   (0.00512041, 'let'),\n",
            "   (0.004595719, 'theorem'),\n",
            "   (0.004200147, 'classifier'),\n",
            "   (0.004078992, 'vc_dimension'),\n",
            "   (0.0038786836, 'sample'),\n",
            "   (0.003864169, 'threshold'),\n",
            "   (0.0036678903, 'lower_bound'),\n",
            "   (0.0035631831, 'upper_bound'),\n",
            "   (0.003503592, 'at_least'),\n",
            "   (0.0033393083, 'xi'),\n",
            "   (0.0033243627, 'decision_tree'),\n",
            "   (0.0030601828, 'dimension'),\n",
            "   (0.0029426457, 'training_set'),\n",
            "   (0.00279046, 'proof'),\n",
            "   (0.002765225, 'log'),\n",
            "   (0.0027459036, 'according_to')],\n",
            "  -1.3205548662147901),\n",
            " ([(0.011689465, 'reinforcement_learning'),\n",
            "   (0.008683697, 'action'),\n",
            "   (0.0071928566, 'policy'),\n",
            "   (0.0058526695, 'reinforcement'),\n",
            "   (0.0053929645, 'state_space'),\n",
            "   (0.004999798, 'optimal'),\n",
            "   (0.004664538, 'dynamic_programming'),\n",
            "   (0.004468938, 'control'),\n",
            "   (0.0039947233, 'mean_field'),\n",
            "   (0.0035031561, 'time_step'),\n",
            "   (0.0034605972, 'markov'),\n",
            "   (0.003330987, 'reward'),\n",
            "   (0.0033000617, 'dynamic'),\n",
            "   (0.003274519, 'machine_learning'),\n",
            "   (0.0032683595, 'source'),\n",
            "   (0.0031166205, 'independent_component'),\n",
            "   (0.002806248, 'approximation'),\n",
            "   (0.0026131696, 'stochastic'),\n",
            "   (0.0025888423, 'environment'),\n",
            "   (0.002584965, 'monte_carlo')],\n",
            "  -1.8801631397422458)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}